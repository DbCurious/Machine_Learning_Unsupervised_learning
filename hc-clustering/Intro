hc-clustering
  Types of Hierarchical Clustering
  There are mainly two types of hierarchical clustering:

    1.Agglomerative hierarchical clustering
    2.Divisive Hierarchical clustering
    
    
    Agglomerative Hierarchical Clustering
      a.We assign each point to an individual cluster in this technique.
      b.Suppose there are 4 data points. We will assign each of these points to a cluster and hence will have 4 clusters in the beginning.
      c.Then, at each iteration, we merge the closest pair of clusters and repeat this step until only a single cluster is left.
      d.We are merging (or adding) the clusters at each step, right? Hence, this type of clustering is also known as additive hierarchical clustering.
        Following are the steps involved in agglomerative clustering:
        1.At the start, treat each data point as one cluster. Therefore, the number of clusters at the start will be K, while K is an integer representing the number of data points.
        2.Form a cluster by joining the two closest data points resulting in K-1 clusters.
        3.Form more clusters by joining the two closest clusters resulting in K-2 clusters.
        4.Repeat the above three steps until one big cluster is formed.
        5.Once single cluster is formed, dendrograms are used to divide into multiple clusters depending upon the problem. We will study the concept of dendrogram in detail in an upcoming section.
      
      #code
      X = np.array([[5,3],
          [10,15],
          [15,12],
          [24,10],
          [30,30],
          [85,70],
          [71,80],
          [60,78],
          [70,55],
          [80,91],])
      from sklearn.cluster import AgglomerativeClustering
      cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')
      cluster.fit_predict(X)
      plt.scatter(X[:,0],X[:,1], c=cluster.labels_, cmap='rainbow')
      
        The number of parameters is set to 2 using the n_clusters parameter while the affinity is set to "euclidean" (distance between the datapoints).
        Linkage parameter is set to "ward", which minimizes the variant between the clusters.
     
    Divisive Hierarchical clustering
      1.It starts by including all objects in a single large cluster. At each step of iteration, the most heterogeneous cluster is divided into two. 
      2.The process is iterated until all objects are in their own cluster.  
      3.It doesnâ€™t matter if we have 10 or 1000 data points. All these points will belong to the same cluster at the beginning.
      4.At each iteration, we split the farthest point in the cluster and repeat this process until each cluster only contains a single point.


    
